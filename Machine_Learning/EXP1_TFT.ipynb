{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_forecasting\\models\\base_model.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_20016\\961043184.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  future_data['total_vacancies'].fillna(0, inplace=True)\n",
      "C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_20016\\961043184.py:45: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  future_data['total_vacancies'].fillna(0, inplace=True)\n",
      "C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_20016\\961043184.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['total_vacancies_scaled'] = scaler.fit_transform(train_data[['total_vacancies']])\n",
      "C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_20016\\961043184.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['geo_encoded'] = geo_encoder.fit_transform(train_data['geo'])\n",
      "C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_20016\\961043184.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['noc_desc_encoded'] = sector_encoder.fit_transform(train_data['noc_desc'])\n",
      "C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_20016\\961043184.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['job_char_encoded'] = job_char_encoder.fit_transform(train_data['job_char'])\n",
      "C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_20016\\961043184.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['time_idx'] = (train_data['ref_date'] - train_data['ref_date'].min()).dt.days\n",
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:1301: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 40 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__geo_encoded': 0, '__group_id__noc_desc_encoded': 3, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 0, '__group_id__noc_desc_encoded': 4, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 0, '__group_id__noc_desc_encoded': 7, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 1, '__group_id__noc_desc_encoded': 2, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 1, '__group_id__noc_desc_encoded': 3, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 1, '__group_id__noc_desc_encoded': 4, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 2, '__group_id__noc_desc_encoded': 4, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 3, '__group_id__noc_desc_encoded': 3, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 3, '__group_id__noc_desc_encoded': 4, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 3, '__group_id__noc_desc_encoded': 5, '__group_id__job_char_encoded': 1}]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder, GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('./MachineLearning.csv')\n",
    "data['ref_date'] = pd.to_datetime(data['ref_date'], format='%d-%m-%Y')\n",
    "\n",
    "# Remove unwanted entries\n",
    "data = data[\n",
    "    (data['geo'] != 'Canada') &\n",
    "    (data['noc_code'] != 101) &\n",
    "    (data['job_char'] != 'Type of work, all types')\n",
    "]\n",
    "\n",
    "# Filter data for training (2015-2022) and future prediction (2023-2024)\n",
    "train_data = data[(data['ref_date'] >= '01-01-2015') & (data['ref_date'] <= '31-12-2022')]\n",
    "future_data = data[(data['ref_date'] >= '01-01-2023') & (data['ref_date'] <= '31-12-2024')]\n",
    "\n",
    "# Generate future dates for 2025-2027\n",
    "future_dates = pd.date_range(start=\"01-01-2025\", end=\"31-12-2027\", freq=\"QS\")\n",
    "\n",
    "# Generate all combinations for future data\n",
    "geo_values = train_data['geo'].unique()\n",
    "noc_desc_values = train_data['noc_desc'].unique()\n",
    "job_char_values = train_data['job_char'].unique()\n",
    "\n",
    "future_combinations = pd.DataFrame(\n",
    "    list(product(future_dates, geo_values, noc_desc_values, job_char_values)),\n",
    "    columns=[\"ref_date\", \"geo\", \"noc_desc\", \"job_char\"]\n",
    ")\n",
    "future_combinations['total_vacancies'] = None\n",
    "\n",
    "# Combine with existing future data\n",
    "future_data = pd.concat([future_data, future_combinations], ignore_index=True)\n",
    "\n",
    "# Fill missing values\n",
    "future_data['total_vacancies'].fillna(0, inplace=True)\n",
    "\n",
    "# Scale target variable\n",
    "scaler = StandardScaler()\n",
    "train_data['total_vacancies_scaled'] = scaler.fit_transform(train_data[['total_vacancies']])\n",
    "future_data['total_vacancies_scaled'] = scaler.transform(future_data[['total_vacancies']])\n",
    "\n",
    "# Encode categorical variables\n",
    "geo_encoder = NaNLabelEncoder()\n",
    "sector_encoder = NaNLabelEncoder()\n",
    "job_char_encoder = NaNLabelEncoder()\n",
    "\n",
    "train_data['geo_encoded'] = geo_encoder.fit_transform(train_data['geo'])\n",
    "train_data['noc_desc_encoded'] = sector_encoder.fit_transform(train_data['noc_desc'])\n",
    "train_data['job_char_encoded'] = job_char_encoder.fit_transform(train_data['job_char'])\n",
    "\n",
    "future_data['geo_encoded'] = geo_encoder.transform(future_data['geo'])\n",
    "future_data['noc_desc_encoded'] = sector_encoder.transform(future_data['noc_desc'])\n",
    "future_data['job_char_encoded'] = job_char_encoder.transform(future_data['job_char'])\n",
    "\n",
    "# Create time index\n",
    "train_data['time_idx'] = (train_data['ref_date'] - train_data['ref_date'].min()).dt.days\n",
    "future_data['time_idx'] = (future_data['ref_date'] - train_data['ref_date'].min()).dt.days\n",
    "\n",
    "# Define TimeSeriesDataSet\n",
    "max_encoder_length = 16\n",
    "max_prediction_length = 8\n",
    "\n",
    "train_dataset = TimeSeriesDataSet(\n",
    "    train_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"total_vacancies_scaled\",\n",
    "    group_ids=[\"geo_encoded\", \"noc_desc_encoded\", \"job_char_encoded\"],\n",
    "    min_encoder_length=1,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_unknown_reals=[\"total_vacancies_scaled\"],\n",
    "    categorical_encoders={\n",
    "        \"geo_encoded\": NaNLabelEncoder(),\n",
    "        \"noc_desc_encoded\": NaNLabelEncoder(),\n",
    "        \"job_char_encoded\": NaNLabelEncoder()\n",
    "    },\n",
    "    target_normalizer=GroupNormalizer(groups=[\"geo_encoded\", \"noc_desc_encoded\", \"job_char_encoded\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = train_dataset.to_dataloader(train=True, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = train_dataset.to_dataloader(train=False, batch_size=64, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type                      | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | tft  | TemporalFusionTransformer | 247 K  | train\n",
      "-----------------------------------------------------------\n",
      "247 K     Trainable params\n",
      "0         Non-trainable params\n",
      "247 K     Total params\n",
      "0.989     Total estimated model params size (MB)\n",
      "244       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 83/83 [00:32<00:00,  2.59it/s, v_num=161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 40. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "Metric val_loss improved. New best score: 0.163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 83/83 [00:56<00:00,  1.47it/s, v_num=161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.023 >= min_delta = 0.001. New best score: 0.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 83/83 [01:00<00:00,  1.37it/s, v_num=161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 0.136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 83/83 [00:56<00:00,  1.47it/s, v_num=161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 0.132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 83/83 [00:53<00:00,  1.55it/s, v_num=161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 0.128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 83/83 [00:54<00:00,  1.53it/s, v_num=161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.128. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 83/83 [00:54<00:00,  1.53it/s, v_num=161]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define and train Temporal Fusion Transformer\n",
    "\n",
    "# Define and train Temporal Fusion Transformer\n",
    "class TFTModule(LightningModule):\n",
    "\tdef __init__(self, tft):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.tft = tft\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\ty_pred = self.tft(x)\n",
    "\t\t# Ensure y_pred has the correct shape\n",
    "\t\tif isinstance(y_pred, tuple):\n",
    "\t\t\ty_pred = y_pred[0]\n",
    "\t\treturn y_pred\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\tx, y = batch\n",
    "\t\ty_hat = self(x)\n",
    "\t\tloss = self.tft.loss(y_hat, y)\n",
    "\t\tself.log(\"train_loss\", loss)\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef validation_step(self, batch, batch_idx):\n",
    "\t\tx, y = batch\n",
    "\t\ty_hat = self(x)\n",
    "\t\tloss = self.tft.loss(y_hat, y)\n",
    "\t\tself.log(\"val_loss\", loss)\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\treturn torch.optim.Adam(self.parameters(), lr=0.03)\n",
    "\n",
    "\n",
    "tft = TFTModule(TemporalFusionTransformer.from_dataset(\n",
    "\ttrain_dataset,\n",
    "\tlearning_rate=0.005,\n",
    "\thidden_size=64,\n",
    "\tattention_head_size=8,\n",
    "\tdropout=0.1,\n",
    "\thidden_continuous_size=32,\n",
    "\toutput_size=len(QuantileLoss().quantiles),  # QuantileLoss output size\n",
    "\tloss=QuantileLoss(),\n",
    "\toptimizer=\"adam\",\n",
    "\tlog_interval=10\n",
    "))\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=True, mode=\"min\", min_delta=0.001)\n",
    "trainer = Trainer(\n",
    "\tmax_epochs=50,  # Increased number of epochs\n",
    "\taccelerator=\"cpu\",\n",
    "\tdevices=1,\n",
    "\tcallbacks=[early_stop_callback],\n",
    "\tlog_every_n_steps=1,  # Log every step\n",
    "    enable_progress_bar=True,\n",
    "    enable_checkpointing=False  # Skip checkpoints to save time\n",
    ")\n",
    "trainer.fit(tft, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:1301: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 70 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__geo_encoded': 0, '__group_id__noc_desc_encoded': 2, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 0, '__group_id__noc_desc_encoded': 3, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 0, '__group_id__noc_desc_encoded': 4, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 1, '__group_id__noc_desc_encoded': 2, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 1, '__group_id__noc_desc_encoded': 3, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 1, '__group_id__noc_desc_encoded': 4, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 2, '__group_id__noc_desc_encoded': 4, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 3, '__group_id__noc_desc_encoded': 2, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 3, '__group_id__noc_desc_encoded': 3, '__group_id__job_char_encoded': 1}, {'__group_id__geo_encoded': 3, '__group_id__noc_desc_encoded': 4, '__group_id__job_char_encoded': 1}]\n",
      "  warnings.warn(\n",
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=17` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "combined_data = pd.concat([train_data, future_data]).drop_duplicates(subset=['time_idx', 'geo_encoded', 'noc_desc_encoded', 'job_char_encoded'])\n",
    "combined_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "combined_dataset = TimeSeriesDataSet.from_dataset(train_dataset, combined_data)\n",
    "dataloader = combined_dataset.to_dataloader(train=False, batch_size=64)\n",
    "\n",
    "# Use the TemporalFusionTransformer model directly for predictions\n",
    "predictions = tft.tft.predict(dataloader, mode=\"quantiles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denormalized predictions saved to 'denormalized_predictions_sanjay.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Denormalize predictions\n",
    "predicted_vacancies = predictions[:, :, 0].numpy().flatten().reshape(-1, 1)\n",
    "denormalized_vacancies = scaler.inverse_transform(predicted_vacancies).flatten()\n",
    "\n",
    "# Ensure the lengths of the arrays are the same\n",
    "min_length = min(len(future_data['ref_date'].values), len(denormalized_vacancies))\n",
    "\n",
    "# Save results\n",
    "results = pd.DataFrame({\n",
    "    \"ref_date\": future_data['ref_date'].values[:min_length],\n",
    "    \"geo\": geo_encoder.inverse_transform(future_data['geo_encoded'].values[:min_length]),\n",
    "    \"noc_desc\": sector_encoder.inverse_transform(future_data['noc_desc_encoded'].values[:min_length]),\n",
    "    \"job_char\": job_char_encoder.inverse_transform(future_data['job_char_encoded'].values[:min_length]),\n",
    "    \"predicted_vacancies\": denormalized_vacancies[:min_length]\n",
    "})\n",
    "results.to_csv(\"NEWWWWWW.csv\", index=False)\n",
    "print(\"Denormalized predictions saved to 'denormalized_predictions_sanjay.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
